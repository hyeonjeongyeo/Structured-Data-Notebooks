{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datatable as dt\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import optuna\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = pd.read_csv('../input/tabular-playground-series-oct-2021/train.csv')\n",
    "test = pd.read_csv('../input/tabular-playground-series-oct-2021/test.csv')\n",
    "sample_submission = pd.read_csv('../input/tabular-playground-series-oct-2021/sample_submission.csv')\n",
    "\n",
    "memory_usage = train.memory_usage(deep=True) / 2 ** 11\n",
    "start_memory = memory_usage.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train.columns.tolist()[1:-1]\n",
    "con_features = train.select_dtypes(include = 'float64').columns.tolist()\n",
    "cat_features = train.select_dtypes(include = 'int64').columns.tolist()[1:-1]\n",
    "\n",
    "train[con_features] = train[con_features].astype('float32')\n",
    "train[cat_features] = train[cat_features].astype('uint8')\n",
    "\n",
    "test[con_features] = test[con_features].astype('float32')\n",
    "test[cat_features] = test[cat_features].astype('uint8')\n",
    "\n",
    "memory_usage = train.memory_usage(deep=True) / 2 ** 11\n",
    "end_memory = memory_usage.sum()\n",
    "\n",
    "print('Memory usage decreased from {:.2f} MB to {:2f} MB ({:.2f} % reduction)'.format(start_memory, end_memory, 100 * (start_memory - end_memory) / start_memory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[\"id\", \"target\"]).copy()\n",
    "y = train[\"target\"].copy()\n",
    "X_test = test.drop(columns=[\"id\"]).copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = pd.DataFrame (data=scaler.fit_transform(X), columns=X.columns)\n",
    "X_test = pd.DataFrame (data=scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model requires big computation, 8 core TPU was used to run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 512\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"Batch Size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tpu_strategy.scope():\n",
    "        \n",
    "    kf = StratifiedKFold(n_splits=7, shuffle=True, random_state=786)\n",
    "\n",
    "    test_preds = []\n",
    "    scores = []\n",
    "\n",
    "    for fold, (idx_train, idx_valid) in enumerate(kf.split(X.iloc[:10000], y[:10000])):\n",
    "\n",
    "        X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n",
    "        X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n",
    "\n",
    "        params['learning_rate']=0.05\n",
    "        model1 = LGBMClassifier(**params)\n",
    "\n",
    "        print('Processing Model1 ...')\n",
    "        model1.fit(X_train,y_train,\n",
    "                   eval_set=[(X_valid,y_valid)],\n",
    "                   eval_metric='auc',\n",
    "                   verbose=False)\n",
    "\n",
    "        params['learning_rate']=0.01\n",
    "        model2 = LGBMClassifier(**params)\n",
    "\n",
    "        print('Processing Model2 ...')\n",
    "        model2.fit(X_train,y_train,\n",
    "                   eval_set=[(X_valid,y_valid)],\n",
    "                   eval_metric='auc',\n",
    "                   verbose=False,\n",
    "                   init_model=model1)\n",
    "\n",
    "        params['learning_rate']=0.007\n",
    "        model3 = LGBMClassifier(**params)\n",
    "\n",
    "        print('Processing Model3 ...')\n",
    "        model3.fit(X_train,y_train,\n",
    "                   eval_set=[(X_valid,y_valid)],\n",
    "                   eval_metric='auc',\n",
    "                   verbose=False,\n",
    "                   init_model=model2)\n",
    "\n",
    "        params['learning_rate']=0.001\n",
    "        model4 = LGBMClassifier(**params)\n",
    "\n",
    "        print('Processing Model4 ...')\n",
    "        model4.fit(X_train,y_train,\n",
    "                   eval_set=[(X_valid,y_valid)],\n",
    "                   eval_metric='auc',\n",
    "                   verbose=False,\n",
    "                   init_model=model3)\n",
    "\n",
    "        pred_valid = model4.predict_proba(X_valid)[:,1]\n",
    "        fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n",
    "        score = auc(fpr, tpr)\n",
    "        scores.append(score)\n",
    "\n",
    "        print(f\"Fold: {fold + 1} Score: {score}\")\n",
    "        print('Predicting test data ...')\n",
    "\n",
    "        test_preds.append(model3.predict_proba(X_test)[:,1])\n",
    "\n",
    "    print(f\"Overall Validation Score: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of median folds had lower score than prediction of mean folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.mean(np.column_stack(test_preds),axis=1)\n",
    "\n",
    "sample_submission['target'] = predictions\n",
    "sample_submission.to_csv('lgbm_sub_mean.csv', index=False)\n",
    "sample_submission.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Reference:\n",
    "\n",
    "- Paper : Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting (https://arxiv.org/abs/1912.09363)\n",
    "- Keras Docs https://keras.io/examples/structured_data/classification_with_grn_and_vsn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    batch_size = strategy.num_replicas_in_sync * 64\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "    print(f\"Batch Size: {batch_size}\")\n",
    "    \n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    batch_size = 256\n",
    "    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n",
    "    print(f\"Batch Size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/kaggle/input/tabular-playground-series-nov-2021/'\n",
    "\n",
    "train = pd.read_csv(file_path + 'train.csv')\n",
    "test = pd.read_csv(file_path + 'test.csv')\n",
    "sub = pd.read_csv(file_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train data shape: {train.shape}')\n",
    "print(f'Test data shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f'f{i}' for i in range(100)]\n",
    "csv_header = feature_names + ['target']\n",
    "target = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling\n",
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train[feature_names])\n",
    "test = scaler.transform(test[feature_names])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "train = pd.DataFrame(train, columns=feature_names)\n",
    "test = pd.DataFrame(test, columns=feature_names)\n",
    "train['target'] = target\n",
    "test['target'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection = np.random.rand(len(train.index)) <= 0.85\n",
    "train_data = train[random_selection]\n",
    "valid_data = train[~random_selection]\n",
    "test_data = test.copy()\n",
    "\n",
    "train_data.to_csv('train_data.csv', index=False, header=False)\n",
    "valid_data.to_csv('valid_data.csv', index=False, header=False)\n",
    "test_data.to_csv('test_data.csv', index=False, header=False)\n",
    "\n",
    "print(f'Train data shape: {train_data.shape}')\n",
    "print(f'Valid data shape: {valid_data.shape}')\n",
    "print(f'Test data shape: {test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=csv_header,\n",
    "        label_name='target',\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in feature_names:\n",
    "        inputs[feature_name] = layers.Input(name=feature_name, shape=(), dtype=tf.float32)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, encoding_size):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        # Project the numeric feature to encoding_size using linear transformation.\n",
    "        encoded_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n",
    "        encoded_features.append(encoded_feature)\n",
    "    return encoded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedLinearUnit(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        self.linear = layers.Dense(units)\n",
    "        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs) * self.sigmoid(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedResidualNetwork(layers.Layer):\n",
    "    def __init__(self, units, dropout_rate):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.units = units\n",
    "        self.elu_dense = layers.Dense(units, activation=\"elu\")\n",
    "        self.linear_dense = layers.Dense(units)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.gated_linear_unit = GatedLinearUnit(units)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        self.project = layers.Dense(units)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.elu_dense(inputs)\n",
    "        x = self.linear_dense(x)\n",
    "        x = self.dropout(x)\n",
    "        if inputs.shape[-1] != self.units:\n",
    "            inputs = self.project(inputs)\n",
    "        x = inputs + self.gated_linear_unit(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableSelection(layers.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate):\n",
    "        super(VariableSelection, self).__init__()\n",
    "        self.grns = list()\n",
    "        # Create a GRN for each feature independently\n",
    "        for idx in range(num_features):\n",
    "            grn = GatedResidualNetwork(units, dropout_rate)\n",
    "            self.grns.append(grn)\n",
    "        # Create a GRN for the concatenation of all the features\n",
    "        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n",
    "        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        v = layers.concatenate(inputs)\n",
    "        v = self.grn_concat(v)\n",
    "        v = tf.expand_dims(self.softmax(v), axis=-1)\n",
    "\n",
    "        x = []\n",
    "        for idx, inp in enumerate(inputs):\n",
    "            x.append(self.grns[idx](inp))\n",
    "        x = tf.stack(x, axis=1)\n",
    "\n",
    "        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(encoding_size):\n",
    "    inputs = create_model_inputs()\n",
    "    feature_list = encode_inputs(inputs, encoding_size)\n",
    "    num_features = len(feature_list)\n",
    "\n",
    "    features = VariableSelection(num_features, encoding_size, dropout_rate)(feature_list)\n",
    "\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "dropout_rate = 0.15\n",
    "batch_size = 256\n",
    "num_epochs = 20\n",
    "encoding_size = 256\n",
    "\n",
    "with strategy.scope():\n",
    "    \n",
    "    model = create_model(encoding_size)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.AUC()],\n",
    "    )\n",
    "\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=20, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    pl = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss', factor=0.1, patience=20, verbose=0,\n",
    "        mode='auto', min_delta=0.0001, cooldown=0, min_lr=0,\n",
    "    )\n",
    "    \n",
    "\n",
    "    print(\"Start training the model ...\")\n",
    "    train_dataset = get_dataset_from_csv('train_data.csv', shuffle=True, batch_size=batch_size)\n",
    "    valid_dataset = get_dataset_from_csv('valid_data.csv', batch_size=batch_size)\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=[es, pl],\n",
    "    )\n",
    "    print(\"Model training Completed.\")\n",
    "\n",
    "    \n",
    "    print(\"Predicting model performance ...\")\n",
    "    test_dataset = get_dataset_from_csv('test_data.csv', batch_size=batch_size)\n",
    "    preds = model.predict(test_dataset)\n",
    "    print(\"Prediction Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = preds\n",
    "sub.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
